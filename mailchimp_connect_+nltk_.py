# -*- coding: utf-8 -*-
"""Mailchimp connect +NLTK .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FNqjTknBygrM95lI6Jn0wLTliJ-NH4J0
"""

pip install mailchimp-marketing

import mailchimp_marketing as MailchimpMarketing
from mailchimp_marketing.api_client import ApiClientError
import pandas as pd

"""#**LDA ANALYSIS**
#PART 1:GETTING SURVEY DATA AND CUSTOMER DATA FROM MAILCHIMP

STEP1: Accessing survey response from Mailchimp
"""

#****#
name = 'The Mondavi Center'  #****replace with desired audience under which the survey will be stored*****
title = 'Post-show Survey (Live version)'  #****replace with desired survey title from mailchimp. Further code extracts survey responses only from this title*****

#First we will extract list_id. 
#list ID will be unique for each auudience (The Mondavi Center, Mondavi arts education and event cancellation)
#we need to extract the list ID for the audience under which the survey is stored

#For this instance, our desired audience is 'The Mondavi Center'

client = MailchimpMarketing.Client()
client.set_config({
    "api_key": "d1dd86849ea5ab988c6019b5bc07166d-us11", #extracted from mondavi's mailchimp account
    "server": "us11"
  })

all_list = client.lists.get_all_lists()



for listi in all_list['lists']:
    if listi['name'] == name:
        list_id = listi['id']
        break

print(list_id)

#next, we will find out the survey id for a specific survey using the title of the survey
surveysinfo = client.lists.get_all_surveys_for_list(list_id) 



for survey in surveysinfo['surveys']:
    if survey['title'] == title:
        survey_id = survey['id']
        break

print(survey_id) #survey_id for your specified survey title

#extracting all the response ids for that particular survey
data_response = client.reporting.get_survey_responses_all(survey_id)#get all responses meta data 

response_ids = [resp.get('response_id') for resp in data_response.get('responses', []) if resp is not None] #extract all the response ids 
print(response_ids)

#using the response ids to extract all the survey responses

responses = [] # list to store all the responses

for response_id in response_ids:
    response = client.reporting.get_survey_response(survey_id, response_id)
    responses.append(response)

# create a dataframe from the responses
df = pd.DataFrame(responses)

#PS- THIS CHUNK TAKES A FEW MINS TO LOAD

"""In the next piece of codes, we will clean the responses dataframe in a desired dataframe or table format, separating response answers in different columns for easy integration with tessitura """

# the values in the contact column are stores as a dictionary. in the next code, we will separate them into different columns
df_contact = pd.json_normalize(df['contact'])

# Join the two DataFrames on the index to keep the other columns in the original DataFrame
dfnew = df.join(df_contact)

# Drop the original 'contact' columns
dfnew = dfnew.drop(['contact'], axis=1)

# Delete columns "avatar_url", "contact_id" and "email_id" from the dataframe as does not give any useful info
#correct email format is already stored under column email
dfnew = dfnew.drop(["avatar_url", "contact_id", "email_id"], axis=1)

# Move the "results" column to the end of the dataframe (not required, just for better visualizing)
cols = list(dfnew.columns)
cols.remove("results")
dfnew = dfnew[cols + ["results"]]


#next we will also format the results column to separate all answers in different columns
#Creating the data frame with a single column 'results'
new_df = dfnew[['results']]

# column names for the new columns from results
dfnew2 = pd.DataFrame(columns=[
    'How satisfied were you with your recent experience at the Mondavi Center?',
    'How likely is it that you would recommend the Mondavi Center to a friend or colleague?',
    'Please feel free to leave any comments about your experience at the Mondavi Center.',
    'Enter your email address:'
])

#next, we will extract only the asnwers from the results column for each of the questions
# Iterating over the rows of the 'results' column
for i in range(len(new_df)):
    row = new_df.loc[i, 'results']
    row_data = {}
    for item in row:
        row_data[item['query']] = item.get('answer', '')
    dfnew2 = dfnew2.append(row_data, ignore_index=True)

#we will format the 'how likely are you to recommend' column to only show score given out of 10 insead of say 8/10
dfnew2['How likely is it that you would recommend the Mondavi Center to a friend or colleague?'] = dfnew2['How likely is it that you would recommend the Mondavi Center to a friend or colleague?'].astype(str).str.extract('(\d+)').astype(int)
 
# Drop the original 'results' columns 
dfnew = dfnew.drop(['results'], axis=1)

# lastly joining the two created dataframes to give the final survey result df
final_surveyresult = dfnew.join(dfnew2)

print(final_surveyresult)

"""STEP2: Now that we have the survey results in the correct format, we will access the mailchimp customer data. Here we are trying to access the Customer ID for each customer in order to later tag each survey response to the appropriate customer ID, using the customer email address as the common key """

#in this chunk, we are using mailchimp API to input the email address from the survey data to retrieve the tessitura customer id 
result_df = pd.DataFrame(columns=['email', 'custid']) # Create an empty DataFrame to store the results

# Loop over each row in the DataFrame
for index, row in final_surveyresult.iterrows():
    
    # Get email from the current row
    if pd.isna(row['email']): #if email column is blank, it will refer to the other column called "Enter your email address:"
        email = row['Enter your email address:']
    else:
        email = row['email']
    
    # Query Mailchimp API to get member information
    try:
        member = client.lists.get_list_member(list_id, subscriber_hash=email.lower())
        custid = member['merge_fields']['CUSTID']
        result_df = result_df.append({'email': email, 'custid': custid}, ignore_index=True)
    except Exception as e:
        print(f"Error getting CUSTID for {email}: {e}")

# Print the resulting DataFrame
print(result_df) #for easy viewing, we create a dataframe with email and CUSTID

# Print the resulting DataFrame
print(result_df) #for easy viewing, we create a dataframe with email and CUSTID

#final step, we will join the survey data with this new dataframe to include cust id for each survey
survey_tessID = pd.merge(final_surveyresult, result_df.drop_duplicates(subset=['email'], keep='first'), on='email', how='left')

survey_tessID.isnull().sum() #There are few (10 as of Apr 20'22) instances where the CUSTID is null. As per Sarah, they are using different email for survey

df = survey_tessID.dropna(subset=['custid']) #We will drop those few rows that do not have customer ID

"""#PART2: SENTIMENT ANALYSIS ON SURVEY DATA"""

!pip install nltk
!pip install textblob

import pandas as pd
import nltk
from textblob import TextBlob

nltk.download('punkt')
nltk.download('stopwords')

#Define a function to perform sentiment analysis using TextBlob:
def get_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity

#Apply the sentiment analysis function to the "reviews" column in your DataFrame to get a sentiment score for each review:
df['sentiment_score'] = df['Please feel free to leave any comments about your experience at the Mondavi Center.'].apply(get_sentiment)

df['sentiment'] = pd.cut(df['sentiment_score'], bins=[-1, -0.1, 0.1, 1], labels=['negative', 'neutral', 'positive'])

!pip install neattext

import neattext.functions as nfx

!pip install scikit-plot

#Estimators
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB

#transformers
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

! pip install transformers -q

from transformers import pipeline
emotion = pipeline('sentiment-analysis', model='arpanghoshal/EmoRoBERTa') #emotion label

def get_emotion_labelScore(text):
  return(emotion(text)[0]['score'])

def get_emotion_label(text):
  return(emotion(text)[0]['label'])

#In this part, we will apply emotion label and score to each review
df['emotion_Label'] = df['Please feel free to leave any comments about your experience at the Mondavi Center.'].apply(get_emotion_label)

df['emotion_Score'] = df['Please feel free to leave any comments about your experience at the Mondavi Center.'].apply(get_emotion_labelScore)

"""#LDA Modeling"""

#Starting by separating positive, negative and neutral reviews so we can identify the topics for each type of response
positive = df.loc[df['sentiment']=='positive']
negative = df.loc[df['sentiment']=='negative']
neutral = df.loc[df['sentiment']=='neutral']

"""## positive reveiws topic modeling"""

texts = positive['Please feel free to leave any comments about your experience at the Mondavi Center.'].tolist()

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Download stopwords
nltk.download('stopwords')
stop_words = stopwords.words('english')

import nltk

nltk.download('punkt')

import pandas as pd
import gensim
from gensim import corpora

# Preprocess the texts into a list of lists of tokens
tokenized_texts = []
for text in texts:
    # Tokenize the text
    tokens = word_tokenize(text)
    
    # Remove stop words and non-alphabetic tokens
    tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]
    
    # Add the tokenized text to the list
    tokenized_texts.append(tokens)

# Create a dictionary from the tokenized texts
dictionary = corpora.Dictionary(tokenized_texts)

corpus = [dictionary.doc2bow(text) for text in tokenized_texts]

# Train the LDA model on the corpus
num_topics = 5
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                            id2word=dictionary,
                                            num_topics=num_topics,
                                            random_state=100,
                                            update_every=1,
                                            chunksize=100,
                                            passes=10,
                                            alpha='auto',
                                            per_word_topics=True)

# Print the topics and their top words
for topic in lda_model.print_topics():
    print(topic)

#displaying topics as a table
import pandas as pd

topics = []
top_words = []

for topic in lda_model.print_topics():
    topics.append(topic[0])
    top_words.append(topic[1])

df = pd.DataFrame({'Topic': topics, 'Top Words': top_words})

# Display the table
print(df)

#visualizing in a word cloud format

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Define a function to generate a word cloud for a given topic
def generate_wordcloud(topic_num, lda_model, num_words=10):
    words = dict(lda_model.show_topic(topic_num, num_words))
    wordcloud = WordCloud(width=800, height=400, background_color="white").generate_from_frequencies(words)
    plt.figure(figsize=(12,6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Topic {topic_num}")
    plt.show()

# Generate a word cloud for each topic
for i in range(num_topics):
    generate_wordcloud(i, lda_model)

import matplotlib.pyplot as plt

# Define a function to generate a bar chart for a given topic
def generate_barchart(topic_num, lda_model, num_words=10):
    words, probs = zip(*lda_model.show_topic(topic_num, num_words))
    plt.figure(figsize=(12,6))
    plt.bar(words, probs)
    plt.title(f"Topic {topic_num}")
    plt.xlabel("Words")
    plt.ylabel("Probability")
    plt.show()

# Generate a bar chart for each topic
for i in range(num_topics):
    generate_barchart(i, lda_model)

"""## negative reveiws topic modeling"""

texts = negative['Please feel free to leave any comments about your experience at the Mondavi Center.'].tolist()

# Preprocess the texts into a list of lists of tokens
tokenized_texts = []
for text in texts:
    # Tokenize the text
    tokens = word_tokenize(text)
    
    # Remove stop words and non-alphabetic tokens
    tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]
    
    # Add the tokenized text to the list
    tokenized_texts.append(tokens)

# Create a dictionary from the tokenized texts
dictionary = corpora.Dictionary(tokenized_texts)

corpus = [dictionary.doc2bow(text) for text in tokenized_texts]

# Train the LDA model on the corpus
num_topics = 5
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                            id2word=dictionary,
                                            num_topics=num_topics,
                                            random_state=100,
                                            update_every=1,
                                            chunksize=100,
                                            passes=10,
                                            alpha='auto',
                                            per_word_topics=True)

# Print the topics and their top words
for topic in lda_model.print_topics():
    print(topic)

import pandas as pd

topics = []
top_words = []

for topic in lda_model.print_topics():
    topics.append(topic[0])
    top_words.append(topic[1])

df2 = pd.DataFrame({'Topic': topics, 'Top Words': top_words})

# Display the table
print(df2)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Define a function to generate a word cloud for a given topic
def generate_wordcloud(topic_num, lda_model, num_words=10):
    words = dict(lda_model.show_topic(topic_num, num_words))
    wordcloud = WordCloud(width=800, height=400, background_color="white").generate_from_frequencies(words)
    plt.figure(figsize=(12,6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Topic {topic_num}")
    plt.show()

# Generate a word cloud for each topic
for i in range(num_topics):
    generate_wordcloud(i, lda_model)

import matplotlib.pyplot as plt

# Define a function to generate a bar chart for a given topic
def generate_barchart(topic_num, lda_model, num_words=10):
    words, probs = zip(*lda_model.show_topic(topic_num, num_words))
    plt.figure(figsize=(12,6))
    plt.bar(words, probs)
    plt.title(f"Topic {topic_num}")
    plt.xlabel("Words")
    plt.ylabel("Probability")
    plt.show()

# Generate a bar chart for each topic
for i in range(num_topics):
    generate_barchart(i, lda_model)